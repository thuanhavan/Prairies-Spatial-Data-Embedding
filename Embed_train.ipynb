{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bfbce29-b4d1-43a3-9adf-49da2cbcba98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 tiles.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ==============================\n",
    "# USER SETTINGS\n",
    "# ==============================\n",
    "\n",
    "# Folder with the GEE-exported composite tiles\n",
    "TILES_DIR = r\"C:\\Users\\hvt632\\Presto_embedded_model\\input_img\"\n",
    "\n",
    "# Output folder for embedding rasters\n",
    "OUT_DIR = os.path.join(TILES_DIR, \"embeddings_64d\")\n",
    "\n",
    "# Embedding model parameters\n",
    "EMBED_DIM   = 64       # size of embedding vector (can change to 128, etc.)\n",
    "HIDDEN_DIM  = 256      # hidden layer size in autoencoder\n",
    "\n",
    "# Training parameters\n",
    "N_TRAIN_PIXELS   = 200_000   # total number of pixels to sample for training\n",
    "BATCH_SIZE_TRAIN = 1024\n",
    "N_EPOCHS         = 50\n",
    "\n",
    "# Inference parameters\n",
    "BATCH_SIZE_INFER = 4096\n",
    "\n",
    "# Use GPU if available\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Random seed (optional, for reproducibility)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ==============================\n",
    "# 1. LIST TILES\n",
    "# ==============================\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "tile_paths = sorted(glob.glob(os.path.join(TILES_DIR, \"*.tif\")))\n",
    "if not tile_paths:\n",
    "    raise RuntimeError(f\"No .tif files found in {TILES_DIR}\")\n",
    "\n",
    "print(f\"Found {len(tile_paths)} tiles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da20b986-c6b4-4d94-a02c-af8dd44f16fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting up to 200000 training pixels...\n",
      "  Reading C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\presto_input_SK_2019_2020_tile_0.tif\n",
      "  Reading C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\presto_input_SK_2019_2020_tile_1.tif\n",
      "  Reading C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\presto_input_SK_2019_2020_tile_2.tif\n",
      "  Reading C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\presto_input_SK_2019_2020_tile_3.tif\n",
      "  Reading C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\presto_input_SK_2019_2020_tile_4.tif\n",
      "  Reading C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\presto_input_SK_2019_2020_tile_5.tif\n",
      "  Reading C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\presto_input_SK_2019_2020_tile_6.tif\n",
      "  Reading C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\presto_input_SK_2019_2020_tile_7.tif\n",
      "  Reading C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\presto_input_SK_2019_2020_tile_8.tif\n",
      "  Reading C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\presto_input_SK_2019_2020_tile_9.tif\n",
      "Collected 104160 pixels for training, feature dimension = 206\n",
      "\n",
      "Feature stats:\n",
      "  mean (first 5): [ 5.0959969e+01 -1.0508010e+02  4.3724963e-01  2.0135725e-02\n",
      "  3.8444003e-01]\n",
      "  std  (first 5): [0.04943888 0.21226498 0.0830062  0.14067341 0.3081268 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================\n",
    "# 2. COLLECT TRAINING PIXELS\n",
    "# ==============================\n",
    "\n",
    "def clean_pixel_matrix(data):\n",
    "    \"\"\"\n",
    "    data: (N_pixels, n_bands) float32\n",
    "    - remove non-finite rows (NaN/inf)\n",
    "    - remove rows that are all zero\n",
    "    \"\"\"\n",
    "    # Ensure float32\n",
    "    data = data.astype(np.float32)\n",
    "\n",
    "    # Non-finite → drop\n",
    "    finite_mask = np.all(np.isfinite(data), axis=1)\n",
    "\n",
    "    # All-zero → drop (likely outside ROI)\n",
    "    nonzero_mask = ~np.all(data == 0, axis=1)\n",
    "\n",
    "    mask = finite_mask & nonzero_mask\n",
    "    return data[mask], mask\n",
    "\n",
    "\n",
    "def collect_training_samples(tile_paths, n_train_pixels):\n",
    "    \"\"\"\n",
    "    Iterates over tiles and collects up to n_train_pixels pixel vectors for training.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    total = 0\n",
    "\n",
    "    print(f\"\\nCollecting up to {n_train_pixels} training pixels...\")\n",
    "\n",
    "    for tile_path in tile_paths:\n",
    "        print(f\"  Reading {tile_path}\")\n",
    "        with rasterio.open(tile_path) as src:\n",
    "            arr = src.read()  # shape: (bands, H, W)\n",
    "\n",
    "        bands, H, W = arr.shape\n",
    "        data = arr.reshape(bands, -1).T  # (N_pixels, bands)\n",
    "\n",
    "        data_clean, mask = clean_pixel_matrix(data)\n",
    "        if data_clean.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        if total + data_clean.shape[0] <= n_train_pixels:\n",
    "            samples.append(data_clean)\n",
    "            total += data_clean.shape[0]\n",
    "        else:\n",
    "            need = n_train_pixels - total\n",
    "            idx = np.random.choice(data_clean.shape[0], size=need, replace=False)\n",
    "            samples.append(data_clean[idx])\n",
    "            total += need\n",
    "            break\n",
    "\n",
    "        if total >= n_train_pixels:\n",
    "            break\n",
    "\n",
    "    if not samples:\n",
    "        raise RuntimeError(\"No valid pixels collected for training.\")\n",
    "\n",
    "    X_train = np.vstack(samples)\n",
    "    print(f\"Collected {X_train.shape[0]} pixels for training, \"\n",
    "          f\"feature dimension = {X_train.shape[1]}\")\n",
    "    return X_train\n",
    "\n",
    "\n",
    "X_train = collect_training_samples(tile_paths, N_TRAIN_PIXELS)\n",
    "\n",
    "# ==============================\n",
    "# 3. PER-FEATURE NORMALIZATION\n",
    "# ==============================\n",
    "\n",
    "# Compute mean & std per feature (ignore NaNs just in case)\n",
    "feat_mean = np.nanmean(X_train, axis=0)\n",
    "feat_std  = np.nanstd(X_train, axis=0)\n",
    "\n",
    "# Prevent division by zero\n",
    "feat_std[feat_std == 0] = 1.0\n",
    "\n",
    "# Normalize training data\n",
    "X_train_norm = (X_train - feat_mean) / feat_std\n",
    "\n",
    "print(\"\\nFeature stats:\")\n",
    "print(\"  mean (first 5):\", feat_mean[:5])\n",
    "print(\"  std  (first 5):\", feat_std[:5])\n",
    "\n",
    "# ==============================\n",
    "# 4. DATASET & MODEL\n",
    "# ==============================\n",
    "\n",
    "class PixelDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        # X: numpy array (N, D)\n",
    "        self.X = torch.from_numpy(X)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx]\n",
    "\n",
    "\n",
    "input_dim = X_train_norm.shape[1]\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_rec = self.decoder(z)\n",
    "        return x_rec, z\n",
    "\n",
    "model = Autoencoder(input_dim, EMBED_DIM, HIDDEN_DIM).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abac49e0-a520-4f06-9ec4-78785a5f6eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training autoencoder...\n",
      "Epoch 1/50 - Loss: 0.276827\n",
      "Epoch 2/50 - Loss: 0.075764\n",
      "Epoch 3/50 - Loss: 0.050439\n",
      "Epoch 4/50 - Loss: 0.037159\n",
      "Epoch 5/50 - Loss: 0.028715\n",
      "Epoch 6/50 - Loss: 0.023744\n",
      "Epoch 7/50 - Loss: 0.020185\n",
      "Epoch 8/50 - Loss: 0.017397\n",
      "Epoch 9/50 - Loss: 0.015361\n",
      "Epoch 10/50 - Loss: 0.013710\n",
      "Epoch 11/50 - Loss: 0.012498\n",
      "Epoch 12/50 - Loss: 0.011516\n",
      "Epoch 13/50 - Loss: 0.010773\n",
      "Epoch 14/50 - Loss: 0.010152\n",
      "Epoch 15/50 - Loss: 0.009529\n",
      "Epoch 16/50 - Loss: 0.009048\n",
      "Epoch 17/50 - Loss: 0.008634\n",
      "Epoch 18/50 - Loss: 0.008178\n",
      "Epoch 19/50 - Loss: 0.007875\n",
      "Epoch 20/50 - Loss: 0.007437\n",
      "Epoch 21/50 - Loss: 0.007251\n",
      "Epoch 22/50 - Loss: 0.006921\n",
      "Epoch 23/50 - Loss: 0.006789\n",
      "Epoch 24/50 - Loss: 0.006570\n",
      "Epoch 25/50 - Loss: 0.006460\n",
      "Epoch 26/50 - Loss: 0.006255\n",
      "Epoch 27/50 - Loss: 0.006143\n",
      "Epoch 28/50 - Loss: 0.006056\n",
      "Epoch 29/50 - Loss: 0.005895\n",
      "Epoch 30/50 - Loss: 0.005918\n",
      "Epoch 31/50 - Loss: 0.005713\n",
      "Epoch 32/50 - Loss: 0.005633\n",
      "Epoch 33/50 - Loss: 0.005547\n",
      "Epoch 34/50 - Loss: 0.005501\n",
      "Epoch 35/50 - Loss: 0.005471\n",
      "Epoch 36/50 - Loss: 0.005471\n",
      "Epoch 37/50 - Loss: 0.005338\n",
      "Epoch 38/50 - Loss: 0.005330\n",
      "Epoch 39/50 - Loss: 0.005155\n",
      "Epoch 40/50 - Loss: 0.005144\n",
      "Epoch 41/50 - Loss: 0.005227\n",
      "Epoch 42/50 - Loss: 0.005043\n",
      "Epoch 43/50 - Loss: 0.005002\n",
      "Epoch 44/50 - Loss: 0.005043\n",
      "Epoch 45/50 - Loss: 0.004976\n",
      "Epoch 46/50 - Loss: 0.004991\n",
      "Epoch 47/50 - Loss: 0.004834\n",
      "Epoch 48/50 - Loss: 0.004821\n",
      "Epoch 49/50 - Loss: 0.004820\n",
      "Epoch 50/50 - Loss: 0.004796\n",
      "Saved model to C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\ae_model_64d_norm.pth\n",
      "\n",
      "Applying encoder to all tiles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing tile: C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\presto_input_SK_2019_2020_tile_0.tif\n",
      "    Valid pixels: 8255 / 250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 1/10 [00:01<00:09,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved embedding tile: C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\embeddings_64d\\presto_input_SK_2019_2020_tile_0_embed_64d.tif\n",
      "  Processing tile: C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\presto_input_SK_2019_2020_tile_1.tif\n",
      "    Valid pixels: 9999 / 250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 2/10 [00:02<00:08,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved embedding tile: C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\embeddings_64d\\presto_input_SK_2019_2020_tile_1_embed_64d.tif\n",
      "  Processing tile: C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\presto_input_SK_2019_2020_tile_2.tif\n",
      "    Valid pixels: 10736 / 250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▉                                                          | 3/10 [00:03<00:07,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved embedding tile: C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\embeddings_64d\\presto_input_SK_2019_2020_tile_2_embed_64d.tif\n",
      "  Processing tile: C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\presto_input_SK_2019_2020_tile_3.tif\n",
      "    Valid pixels: 11199 / 250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 4/10 [00:04<00:06,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved embedding tile: C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\embeddings_64d\\presto_input_SK_2019_2020_tile_3_embed_64d.tif\n",
      "  Processing tile: C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\presto_input_SK_2019_2020_tile_4.tif\n",
      "    Valid pixels: 11500 / 250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [00:05<00:05,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved embedding tile: C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\embeddings_64d\\presto_input_SK_2019_2020_tile_4_embed_64d.tif\n",
      "  Processing tile: C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\presto_input_SK_2019_2020_tile_5.tif\n",
      "    Valid pixels: 11500 / 250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [00:06<00:04,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved embedding tile: C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\embeddings_64d\\presto_input_SK_2019_2020_tile_5_embed_64d.tif\n",
      "  Processing tile: C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\presto_input_SK_2019_2020_tile_6.tif\n",
      "    Valid pixels: 11199 / 250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [00:07<00:03,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved embedding tile: C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\embeddings_64d\\presto_input_SK_2019_2020_tile_6_embed_64d.tif\n",
      "  Processing tile: C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\presto_input_SK_2019_2020_tile_7.tif\n",
      "    Valid pixels: 10736 / 250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [00:08<00:02,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved embedding tile: C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\embeddings_64d\\presto_input_SK_2019_2020_tile_7_embed_64d.tif\n",
      "  Processing tile: C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\presto_input_SK_2019_2020_tile_8.tif\n",
      "    Valid pixels: 9999 / 250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [00:09<00:01,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved embedding tile: C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\embeddings_64d\\presto_input_SK_2019_2020_tile_8_embed_64d.tif\n",
      "  Processing tile: C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\presto_input_SK_2019_2020_tile_9.tif\n",
      "    Valid pixels: 9037 / 250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:10<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved embedding tile: C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\embeddings_64d\\presto_input_SK_2019_2020_tile_9_embed_64d.tif\n",
      "\n",
      "DONE. All embedding tiles saved in:\n",
      "C:\\Users\\hvt632\\Presto_embedded_model\\input_img\\embeddings_64d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 5. TRAIN AUTOENCODER\n",
    "# ==============================\n",
    "\n",
    "print(\"\\nTraining autoencoder...\")\n",
    "train_ds = PixelDataset(X_train_norm)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_dl:\n",
    "        batch = batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        recon, z = model(batch)\n",
    "        loss = loss_fn(recon, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_ds)\n",
    "    print(f\"Epoch {epoch+1}/{N_EPOCHS} - Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# Save model + normalization stats\n",
    "model_path = os.path.join(TILES_DIR, f\"ae_model_{EMBED_DIM}d_norm.pth\")\n",
    "torch.save({\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"input_dim\": input_dim,\n",
    "    \"embed_dim\": EMBED_DIM,\n",
    "    \"hidden_dim\": HIDDEN_DIM,\n",
    "    \"feat_mean\": feat_mean,\n",
    "    \"feat_std\": feat_std\n",
    "}, model_path)\n",
    "print(f\"Saved model to {model_path}\")\n",
    "\n",
    "# ==============================\n",
    "# 6. APPLY MODEL TO EACH TILE\n",
    "# ==============================\n",
    "\n",
    "def compute_embeddings_for_tile(tile_path, model, feat_mean, feat_std, out_dir):\n",
    "    \"\"\"\n",
    "    Loads one composite tile, encodes all valid pixels into embeddings,\n",
    "    and writes an embedding raster with EMBED_DIM bands.\n",
    "    Uses the same normalization (feat_mean/std) as training.\n",
    "    \"\"\"\n",
    "    base = os.path.basename(tile_path)\n",
    "    name, ext = os.path.splitext(base)\n",
    "    out_path = os.path.join(out_dir, f\"{name}_embed_{EMBED_DIM}d.tif\")\n",
    "\n",
    "    if os.path.exists(out_path):\n",
    "        print(f\"  [SKIP] {out_path} already exists\")\n",
    "        return\n",
    "\n",
    "    print(f\"  Processing tile: {tile_path}\")\n",
    "    with rasterio.open(tile_path) as src:\n",
    "        arr = src.read().astype(np.float32)  # (bands, H, W)\n",
    "        profile = src.profile\n",
    "\n",
    "    bands, H, W = arr.shape\n",
    "    data = arr.reshape(bands, -1).T  # (N_pixels, bands)\n",
    "\n",
    "    # Clean as in training\n",
    "    data_clean, mask_valid = clean_pixel_matrix(data)\n",
    "    n_valid = data_clean.shape[0]\n",
    "    print(f\"    Valid pixels: {n_valid} / {data.shape[0]}\")\n",
    "\n",
    "    if n_valid == 0:\n",
    "        print(\"    No valid pixels, skipping.\")\n",
    "        return\n",
    "\n",
    "    # Normalize using training stats\n",
    "    data_norm = (data_clean - feat_mean) / feat_std\n",
    "\n",
    "    # Compute embeddings\n",
    "    model.eval()\n",
    "    emb_valid = np.zeros((n_valid, EMBED_DIM), dtype=np.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, n_valid, BATCH_SIZE_INFER):\n",
    "            batch_np = data_norm[i:i+BATCH_SIZE_INFER]\n",
    "            batch = torch.from_numpy(batch_np).to(DEVICE)\n",
    "            z = model.encoder(batch)\n",
    "            emb_valid[i:i+BATCH_SIZE_INFER] = z.cpu().numpy()\n",
    "\n",
    "    # Fill full grid with 0 for invalid pixels (can treat as nodata later)\n",
    "    emb_full = np.zeros((data.shape[0], EMBED_DIM), dtype=np.float32)\n",
    "    emb_full[mask_valid] = emb_valid\n",
    "\n",
    "    # Reshape to raster format: (bands, H, W)\n",
    "    emb_raster = emb_full.T.reshape(EMBED_DIM, H, W)\n",
    "\n",
    "    # Output profile\n",
    "    out_profile = copy.deepcopy(profile)\n",
    "    out_profile.update({\n",
    "        \"count\": EMBED_DIM,\n",
    "        \"dtype\": \"float32\",\n",
    "        \"nodata\": 0.0    # all-zero vector = invalid pixel\n",
    "    })\n",
    "\n",
    "    with rasterio.open(out_path, \"w\", **out_profile) as dst:\n",
    "        dst.write(emb_raster)\n",
    "\n",
    "    print(f\"    Saved embedding tile: {out_path}\")\n",
    "\n",
    "\n",
    "print(\"\\nApplying encoder to all tiles...\")\n",
    "for tile_path in tqdm(tile_paths):\n",
    "    compute_embeddings_for_tile(tile_path, model, feat_mean, feat_std, OUT_DIR)\n",
    "\n",
    "print(\"\\nDONE. All embedding tiles saved in:\")\n",
    "print(OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
